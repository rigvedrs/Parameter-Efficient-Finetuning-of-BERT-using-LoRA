{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":98084,"databundleVersionId":11711500,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets evaluate accelerate peft trl bitsandbytes\n!pip install nvidia-ml-py3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T01:23:59.437136Z","iopub.execute_input":"2025-04-19T01:23:59.437676Z","iopub.status.idle":"2025-04-19T01:25:26.587649Z","shell.execute_reply.started":"2025-04-19T01:23:59.437653Z","shell.execute_reply":"2025-04-19T01:25:26.586914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, TrainingArguments, Trainer\nfrom transformers import DataCollatorWithPadding\nfrom peft import LoraConfig, get_peft_model, PeftModel\nfrom datasets import load_dataset, Dataset\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom kaggle_secrets import UserSecretsClient\nimport wandb\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb\")\nwandb.login(key=secret_value_0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T01:01:39.083748Z","iopub.execute_input":"2025-04-19T01:01:39.084060Z","iopub.status.idle":"2025-04-19T01:01:44.770409Z","shell.execute_reply.started":"2025-04-19T01:01:39.084036Z","shell.execute_reply":"2025-04-19T01:01:44.769830Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Set random seeds for reproducibility\ndef set_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    \nset_seed(42)\n\n# 1. Load and prepare the AG News dataset\ndataset = load_dataset('ag_news')\nprint(f\"Training set size: {len(dataset['train'])}\")\nprint(f\"Test set size: {len(dataset['test'])}\")\n\n# Get class information\nnum_labels = dataset['train'].features['label'].num_classes\nclass_names = dataset['train'].features['label'].names\nprint(f\"Classes: {class_names}\")\n\n# 2. Initialize tokenizer with improved settings\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\n# 3. Enhanced preprocessing with dynamic max length based on data distribution\ndef analyze_text_lengths(dataset, sample_size=10000):\n    lengths = []\n    for i in range(min(sample_size, len(dataset))):\n        lengths.append(len(tokenizer.encode(dataset[i]['text'])))\n    \n    p95 = np.percentile(lengths, 95)\n    return int(p95)\n\n# Sample the dataset to determine optimal sequence length\noptimal_length = analyze_text_lengths(dataset['train'])\nprint(f\"Optimal sequence length (95th percentile): {optimal_length}\")\n# max_length = min(512, optimal_length)  # Cap at 512 tokens\nmax_length = 128\n\ndef preprocess_function(examples):\n    return tokenizer(\n        examples['text'],\n        truncation=True,\n        max_length=max_length,\n        return_tensors=None\n    )\n\n# Process the dataset\ntokenized_train = dataset['train'].map(preprocess_function, batched=True, remove_columns=['text'])\ntokenized_test = dataset['test'].map(preprocess_function, batched=True, remove_columns=['text'])\n\n# Rename 'label' to 'labels' for model compatibility\ntokenized_train = tokenized_train.rename_column('label', 'labels')\ntokenized_test = tokenized_test.rename_column('label', 'labels')\n\n# Create validation split\ntokenized_train, tokenized_val = tokenized_train.train_test_split(test_size=0.1, seed=42).values()\n\n# 4. Create data collator for efficient batching\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n\n# 5. Initialize base model\nid2label = {i: label for i, label in enumerate(class_names)}\nlabel2id = {label: i for i, label in enumerate(class_names)}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T01:01:44.771687Z","iopub.execute_input":"2025-04-19T01:01:44.772432Z","iopub.status.idle":"2025-04-19T01:02:53.488505Z","shell.execute_reply.started":"2025-04-19T01:01:44.772412Z","shell.execute_reply":"2025-04-19T01:02:53.487921Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_model = RobertaForSequenceClassification.from_pretrained(\n    'roberta-base',\n    num_labels=num_labels,\n    id2label=id2label,\n    label2id=label2id\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T01:03:27.263990Z","iopub.execute_input":"2025-04-19T01:03:27.264671Z","iopub.status.idle":"2025-04-19T01:03:27.522250Z","shell.execute_reply.started":"2025-04-19T01:03:27.264639Z","shell.execute_reply":"2025-04-19T01:03:27.521548Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6. Optimized LoRA Configuration\n# Calculate maximum rank possible within parameter budget\ndef calculate_max_rank(lora_targets, param_budget=1_000_000):\n    total_params = 0\n    ranks = {}\n    \n    # Calculate base parameters for each target\n    for target in lora_targets:\n        if 'query' in target or 'key' in target or 'value' in target:\n            d_in = d_out = 768\n        elif 'intermediate.dense' in target:\n            d_in = 768\n            d_out = 3072\n        elif 'output.dense' in target:\n            d_in = 3072\n            d_out = 768\n        elif 'classifier.dense' in target:\n            d_in = d_out = 768\n        elif 'classifier.out_proj' in target:\n            d_in = 768\n            d_out = num_labels\n        else:\n            d_in = d_out = 768  # Default assumption\n            \n        # For each parameter matrix, we have r*(d_in + d_out) parameters with LoRA\n        ranks[target] = 16  # Initial guess\n        total_params += 12 * ranks[target] * (d_in + d_out)  # 12 layers\n    \n    # Adjust ranks to fit within budget\n    if total_params > param_budget:\n        scale_factor = param_budget / total_params\n        for target in ranks:\n            ranks[target] = max(1, int(ranks[target] * scale_factor))\n    \n    return ranks\n\n# Define target modules to apply LoRA\nlora_targets = [\n    \"query\", \"key\",\"value\"  # Attention matrices\n    # \"intermediate.dense\",      # Feedforward up-projection\n    # \"output.dense\",           # Feedforward down-projection\n    # \"classifier.dense\",       # Classification head\n    # \"classifier.out_proj\"     # Final projection\n]\n\n# Calculate optimal ranks for each target\ntarget_ranks = calculate_max_rank(lora_targets)\nbase_rank = min(target_ranks.values())  # Use the smallest rank for simplicity\nprint(f\"base rank: {base_rank}\")\n\n# Define optimized LoRA configuration\npeft_config = LoraConfig(\n    r=6,\n    lora_alpha=14,  # Higher scale parameter for stronger updates\n    lora_dropout=0.1,  # Increased dropout for regularization\n    bias=\"lora_only\",  # Train bias terms alongside LoRA matrices\n    target_modules=lora_targets,\n    task_type=\"SEQ_CLS\",\n    fan_in_fan_out=False\n)\n\n# Create the LoRA model\npeft_model = get_peft_model(base_model, peft_config)\nprint(\"\\nTrainable parameters information:\")\npeft_model.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T01:03:50.842883Z","iopub.execute_input":"2025-04-19T01:03:50.843685Z","iopub.status.idle":"2025-04-19T01:03:50.885293Z","shell.execute_reply.started":"2025-04-19T01:03:50.843659Z","shell.execute_reply":"2025-04-19T01:03:50.884553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 7. Define evaluation metrics\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n    acc = accuracy_score(labels, preds)\n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }\n\n# 8. Define optimized training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"results/ag_news_roberta_lora\",\n    eval_strategy=\"steps\",\n    eval_steps=200,\n    save_strategy=\"steps\",\n    save_steps=200,\n    logging_steps=50,\n    learning_rate=5e-04,\n    lr_scheduler_type=\"linear\",  # Cosine scheduler with warmup\n    warmup_ratio=0.06,\n    num_train_epochs=1,  # Train longer\n    per_device_train_batch_size=16,  # Larger batch size\n    per_device_eval_batch_size=64,\n    weight_decay=0.01,\n    # fp16=True,  # Use mixed precision\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    greater_is_better=True,\n    # gradient_accumulation_steps=2,  # Accumulate gradients\n    remove_unused_columns=True,\n    group_by_length=True,  # Improve efficiency with similar-length samples\n    optim=\"adamw_torch_fused\",  # Use fused optimizer\n    # early_stopping_patience=1\n)\n\nfrom transformers import EarlyStoppingCallback\n\n# 9. Initialize and run trainer\ntrainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]  # Add this line\n)\n\n# Train the model\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T01:03:56.628935Z","iopub.execute_input":"2025-04-19T01:03:56.629237Z","iopub.status.idle":"2025-04-19T01:19:46.412557Z","shell.execute_reply.started":"2025-04-19T01:03:56.629217Z","shell.execute_reply":"2025-04-19T01:19:46.412006Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 10. Evaluate on test set\n# test_results = trainer.evaluate(tokenized_test)\n# print(f\"Test accuracy: {test_results['eval_accuracy']:.4f}\")\n\n# # 11. Save the model\n# trainer.save_model(\"final_model\")\n\n# # 12. Function to run inference on new data\n# def predict_class(text, model=peft_model, tokenizer=tokenizer):\n#     inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n#     inputs = {k: v.to(model.device) for k, v in inputs.items()}\n    \n#     with torch.no_grad():\n#         outputs = model(**inputs)\n    \n#     logits = outputs.logits\n#     predicted_class_id = logits.argmax(-1).item()\n#     return id2label[predicted_class_id]\n\n# # Example inference\n# example_texts = [\n#     \"New trade agreement between US and China set to boost global markets\",\n#     \"Manchester United wins 2-0 against Liverpool in Premier League clash\",\n#     \"Tech giant announces new smartphone with revolutionary camera system\",\n#     \"Scientists discover potential vaccine for previously incurable disease\"\n# ]\n\n# for text in example_texts:\n#     print(f\"Text: {text[:50]}...\\nPredicted class: {predict_class(text)}\\n\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T01:23:32.844019Z","iopub.execute_input":"2025-04-19T01:23:32.844742Z","iopub.status.idle":"2025-04-19T01:23:32.849165Z","shell.execute_reply.started":"2025-04-19T01:23:32.844718Z","shell.execute_reply":"2025-04-19T01:23:32.848571Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 13. Inference on test set for submission\n# def generate_submission(model, test_dataset, output_file=\"submission.csv\"):\n#     # Create test dataloader\n#     test_dataloader = torch.utils.data.DataLoader(\n#         test_dataset, \n#         batch_size=64, \n#         collate_fn=data_collator\n#     )\n    \n#     model.eval()\n#     all_preds = []\n    \n#     for batch in test_dataloader:\n#         batch = {k: v.to(model.device) for k, v in batch.items()}\n#         with torch.no_grad():\n#             outputs = model(**batch)\n        \n#         preds = outputs.logits.argmax(dim=-1).cpu().numpy()\n#         all_preds.extend(preds)\n    \n#     # Create submission file\n#     submission_df = pd.DataFrame({\n#         'id': range(len(all_preds)),\n#         'label': all_preds\n#     })\n    \n#     submission_df.to_csv(output_file, index=False)\n#     print(f\"Submission file created: {output_file}\")\n\n# # Generate submission file\n# generate_submission(peft_model, tokenized_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T01:23:33.017515Z","iopub.execute_input":"2025-04-19T01:23:33.017710Z","iopub.status.idle":"2025-04-19T01:23:33.022077Z","shell.execute_reply.started":"2025-04-19T01:23:33.017695Z","shell.execute_reply":"2025-04-19T01:23:33.021303Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport evaluate\nfrom tqdm import tqdm\n\ndef evaluate_model(inference_model, dataset, labelled=True, batch_size=8, data_collator=None):\n    \"\"\"\n    Evaluate a PEFT model on a dataset.\n\n    Args:\n        inference_model: The model to evaluate.\n        dataset: The dataset (Hugging Face Dataset) to run inference on.\n        labelled (bool): If True, the dataset includes labels and metrics will be computed.\n                         If False, only predictions will be returned.\n        batch_size (int): Batch size for inference.\n        data_collator: Function to collate batches. If None, the default collate_fn is used.\n\n    Returns:\n        If labelled is True, returns a tuple (metrics, predictions)\n        If labelled is False, returns the predictions.\n    \"\"\"\n    # Create the DataLoader\n    eval_dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=data_collator)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    inference_model.to(device)\n    inference_model.eval()\n\n    all_predictions = []\n    if labelled:\n        metric = evaluate.load('accuracy')\n\n    # Loop over the DataLoader\n    for batch in tqdm(eval_dataloader):\n        # Move each tensor in the batch to the device\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            outputs = inference_model(**batch)\n        predictions = outputs.logits.argmax(dim=-1)\n        all_predictions.append(predictions.cpu())\n\n        if labelled:\n            # Expecting that labels are provided under the \"labels\" key.\n            references = batch[\"labels\"]\n            metric.add_batch(\n                predictions=predictions.cpu().numpy(),\n                references=references.cpu().numpy()\n            )\n\n    # Concatenate predictions from all batches\n    all_predictions = torch.cat(all_predictions, dim=0)\n\n    if labelled:\n        eval_metric = metric.compute()\n        print(\"Evaluation Metric:\", eval_metric)\n        return eval_metric, all_predictions\n    else:\n        return all_predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T01:25:37.296040Z","iopub.execute_input":"2025-04-19T01:25:37.296541Z","iopub.status.idle":"2025-04-19T01:25:37.588428Z","shell.execute_reply.started":"2025-04-19T01:25:37.296515Z","shell.execute_reply":"2025-04-19T01:25:37.587912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Load your unlabelled data\nunlabelled_dataset = pd.read_pickle(\"/kaggle/input/deep-learning-spring-2025-project-2/test_unlabelled.pkl\")\ntest_dataset = unlabelled_dataset.map(preprocess_function, batched=True, remove_columns=[\"text\"])\nunlabelled_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T01:25:37.925162Z","iopub.execute_input":"2025-04-19T01:25:37.925759Z","iopub.status.idle":"2025-04-19T01:25:43.468356Z","shell.execute_reply.started":"2025-04-19T01:25:37.925737Z","shell.execute_reply":"2025-04-19T01:25:43.467785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_dir = \"/kaggle/working/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T01:25:43.469541Z","iopub.execute_input":"2025-04-19T01:25:43.469761Z","iopub.status.idle":"2025-04-19T01:25:43.474051Z","shell.execute_reply.started":"2025-04-19T01:25:43.469745Z","shell.execute_reply":"2025-04-19T01:25:43.473365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run inference and save predictions\npreds = evaluate_model(peft_model, test_dataset, False, 8, data_collator)\ndf_output = pd.DataFrame({\n    'ID': range(len(preds)),\n    'Label': preds.numpy()  # or preds.tolist()\n})\ndf_output.to_csv(os.path.join(output_dir,\"inference_output.csv\"), index=False)\nprint(\"Inference complete. Predictions saved to inference_output.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T01:25:43.474710Z","iopub.execute_input":"2025-04-19T01:25:43.474880Z","iopub.status.idle":"2025-04-19T01:26:41.170508Z","shell.execute_reply.started":"2025-04-19T01:25:43.474866Z","shell.execute_reply":"2025-04-19T01:26:41.169815Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_output.to_csv(os.path.join(output_dir,\"inference_output_no_padding.csv\"), index=False)\nprint(\"Inference complete. Predictions saved to inference_output_no_padding.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T01:27:00.486170Z","iopub.execute_input":"2025-04-19T01:27:00.486879Z","iopub.status.idle":"2025-04-19T01:27:00.499046Z","shell.execute_reply.started":"2025-04-19T01:27:00.486849Z","shell.execute_reply":"2025-04-19T01:27:00.498415Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}