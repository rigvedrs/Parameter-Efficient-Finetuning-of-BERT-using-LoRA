{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":98084,"databundleVersionId":11711500,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets evaluate accelerate peft trl bitsandbytes\n!pip install nvidia-ml-py3","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, TrainingArguments, Trainer\nfrom transformers import DataCollatorWithPadding\nfrom peft import LoraConfig, get_peft_model, PeftModel\nfrom datasets import load_dataset, Dataset\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom kaggle_secrets import UserSecretsClient\nimport wandb\n\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb\")\nwandb.login(key=secret_value_0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set random seeds for reproducibility\ndef set_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    \nset_seed(42)\n\n# 1. Load and prepare the AG News dataset\ndataset = load_dataset('ag_news')\nprint(f\"Training set size: {len(dataset['train'])}\")\nprint(f\"Test set size: {len(dataset['test'])}\")\n\n# Get class information\nnum_labels = dataset['train'].features['label'].num_classes\nclass_names = dataset['train'].features['label'].names\nprint(f\"Classes: {class_names}\")\n\n# 2. Initialize tokenizer with improved settings\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\n# 3. Enhanced preprocessing with dynamic max length based on data distribution\ndef analyze_text_lengths(dataset, sample_size=10000):\n    lengths = []\n    for i in range(min(sample_size, len(dataset))):\n        lengths.append(len(tokenizer.encode(dataset[i]['text'])))\n    \n    p95 = np.percentile(lengths, 95)\n    return int(p95)\n\n# Sample the dataset to determine optimal sequence length\noptimal_length = analyze_text_lengths(dataset['train'])\nprint(f\"Optimal sequence length (95th percentile): {optimal_length}\")\n# max_length = min(512, optimal_length)  # Cap at 512 tokens\nmax_length = 128\n\ndef preprocess_function(examples):\n    return tokenizer(\n        examples['text'],\n        truncation=True,\n        max_length=max_length,\n        return_tensors=None\n    )\n\n# Process the dataset\ntokenized_train = dataset['train'].map(preprocess_function, batched=True, remove_columns=['text'])\ntokenized_test = dataset['test'].map(preprocess_function, batched=True, remove_columns=['text'])\n\n# Rename 'label' to 'labels' for model compatibility\ntokenized_train = tokenized_train.rename_column('label', 'labels')\ntokenized_test = tokenized_test.rename_column('label', 'labels')\n\n# Create validation split\ntokenized_train, tokenized_val = tokenized_train.train_test_split(test_size=0.1, seed=42).values()\n\n# 4. Create data collator for efficient batching\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n\n# 5. Initialize base model\nid2label = {i: label for i, label in enumerate(class_names)}\nlabel2id = {label: i for i, label in enumerate(class_names)}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_model = RobertaForSequenceClassification.from_pretrained(\n    'roberta-base',\n    num_labels=num_labels,\n    id2label=id2label,\n    label2id=label2id\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6. Optimized LoRA Configuration\n\n# Define target modules to apply LoRA\nlora_targets = [\n    \"query\", \"key\",\"value\"  # Attention matrices\n]\n\n# Define optimized LoRA configuration\npeft_config = LoraConfig(\n    r=6,\n    lora_alpha=14,  # Higher scale parameter for stronger updates\n    lora_dropout=0.1,  # Increased dropout for regularization\n    bias=\"lora_only\",  # Train bias terms alongside LoRA matrices\n    target_modules=lora_targets,\n    task_type=\"SEQ_CLS\",\n    fan_in_fan_out=False\n)\n\n# Create the LoRA model\npeft_model = get_peft_model(base_model, peft_config)\nprint(\"\\nTrainable parameters information:\")\npeft_model.print_trainable_parameters()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 7. Define evaluation metrics\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n    acc = accuracy_score(labels, preds)\n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }\n\n# 8. Define optimized training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"results/ag_news_roberta_lora\",\n    eval_strategy=\"steps\",  # Evaluates model at specified step intervals\n    eval_steps=200,\n    save_strategy=\"steps\",  # Saves model at specified step intervals\n    save_steps=200,\n    logging_steps=50,\n    learning_rate=5e-04,\n    lr_scheduler_type=\"linear\",  # Gradually decreases learning rate linearly\n    warmup_ratio=0.06,  # Warms up learning rate for first 6% of training\n    num_train_epochs=1,  # Number of complete passes through the dataset\n    per_device_train_batch_size=16,  # Number of samples per batch during training\n    per_device_eval_batch_size=64,  # Number of samples per batch during evaluation\n    weight_decay=0.01,  # L2 regularization to prevent overfitting\n    # fp16=True,  # Enable half-precision floating point for faster training\n    load_best_model_at_end=True,  # Loads the model with best evaluation metric\n    metric_for_best_model=\"accuracy\",  # Uses accuracy to determine best model\n    greater_is_better=True,  # Higher accuracy is better\n    # gradient_accumulation_steps=2,  # Accumulate gradients over multiple batches\n    remove_unused_columns=True,  # Removes columns not used by model for efficiency\n    group_by_length=True,  # Groups sequences of similar lengths to minimize padding\n    optim=\"adamw_torch_fused\",  # Uses optimized AdamW implementation\n    # early_stopping_patience=1  # Stop training if no improvement after 1 evaluation\n)\n\nfrom transformers import EarlyStoppingCallback\n\n# 9. Initialize and run trainer\ntrainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n)\n\n# Train the model\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport evaluate\nfrom tqdm import tqdm\n\ndef evaluate_model(inference_model, dataset, labelled=True, batch_size=8, data_collator=None):\n    \"\"\"\n    Evaluate a PEFT model on a dataset.\n\n    Args:\n        inference_model: The model to evaluate.\n        dataset: The dataset (Hugging Face Dataset) to run inference on.\n        labelled (bool): If True, the dataset includes labels and metrics will be computed.\n                         If False, only predictions will be returned.\n        batch_size (int): Batch size for inference.\n        data_collator: Function to collate batches. If None, the default collate_fn is used.\n\n    Returns:\n        If labelled is True, returns a tuple (metrics, predictions)\n        If labelled is False, returns the predictions.\n    \"\"\"\n    # Create the DataLoader\n    eval_dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=data_collator)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    inference_model.to(device)\n    inference_model.eval()\n\n    all_predictions = []\n    if labelled:\n        metric = evaluate.load('accuracy')\n\n    # Loop over the DataLoader\n    for batch in tqdm(eval_dataloader):\n        # Move each tensor in the batch to the device\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            outputs = inference_model(**batch)\n        predictions = outputs.logits.argmax(dim=-1)\n        all_predictions.append(predictions.cpu())\n\n        if labelled:\n            # Expecting that labels are provided under the \"labels\" key.\n            references = batch[\"labels\"]\n            metric.add_batch(\n                predictions=predictions.cpu().numpy(),\n                references=references.cpu().numpy()\n            )\n\n    # Concatenate predictions from all batches\n    all_predictions = torch.cat(all_predictions, dim=0)\n\n    if labelled:\n        eval_metric = metric.compute()\n        print(\"Evaluation Metric:\", eval_metric)\n        return eval_metric, all_predictions\n    else:\n        return all_predictions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Load unlabelled data\nunlabelled_dataset = pd.read_pickle(\"/kaggle/input/deep-learning-spring-2025-project-2/test_unlabelled.pkl\")\ntest_dataset = unlabelled_dataset.map(preprocess_function, batched=True, remove_columns=[\"text\"])\nunlabelled_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_dir = \"/kaggle/working/\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run inference and save predictions\npreds = evaluate_model(peft_model, test_dataset, False, 8, data_collator)\ndf_output = pd.DataFrame({\n    'ID': range(len(preds)),\n    'Label': preds.numpy()  # or preds.tolist()\n})\ndf_output.to_csv(os.path.join(output_dir,\"inference_output.csv\"), index=False)\nprint(\"Inference complete. Predictions saved to inference_output.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_output.to_csv(os.path.join(output_dir,\"inference_output_no_padding.csv\"), index=False)\nprint(\"Inference complete. Predictions saved to inference_output_no_padding.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}